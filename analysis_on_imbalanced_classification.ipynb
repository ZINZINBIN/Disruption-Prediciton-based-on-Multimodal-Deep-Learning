{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zinzinbin/.conda/envs/research-env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# use model as ViViT model or slowfast model\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from src.dataloader import VideoDataset\n",
    "from src.utils.sampler import ImbalancedDatasetSampler\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video vision transformer model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from typing import Union, Optional\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from pytorch_model_summary import summary\n",
    "\n",
    "# Module for ViViT\n",
    "# Residual module\n",
    "class Residule(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super(Residule, self).__init__()\n",
    "        self.fn = fn\n",
    "    def forward(self, x : torch.Tensor, **kwargs)->torch.Tensor:\n",
    "        return self.fn(x, **kwargs) + x\n",
    "\n",
    "# PreNorm module\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim : int, fn : Union[nn.Module, None]):\n",
    "        super(PreNorm, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.fn = fn\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "    \n",
    "    def forward(self, x : torch.Tensor, **kwargs)->torch.Tensor:\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "# FeedForward module\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim : int, hidden_dim : int, dropout : float = 0.5):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = dropout\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x : torch.Tensor)->torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "# Attention\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim : int, n_heads : int = 8, d_head : int = 64, dropout : float = 0.5):\n",
    "        super(Attention, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_head\n",
    "        self.dropout = dropout\n",
    "\n",
    "        project_out = not (n_heads == 1 and d_head == dim)\n",
    "\n",
    "        self.inner_dim = d_head * n_heads\n",
    "        self.scale = d_head ** (-0.5)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, self.inner_dim * 3, bias = False) # q, k, v : (dim, inner_dim) matrix\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(self.inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x : torch.Tensor)->torch.Tensor:\n",
    "        b, n, _, h = *x.shape, self.n_heads\n",
    "\n",
    "        qkv = self.to_qkv(x)\n",
    "        qkv = torch.chunk(qkv, 3, -1)\n",
    "\n",
    "        q,k,v = map(lambda t : rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
    "        dots = torch.einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        # obtain attention score\n",
    "        attn = torch.softmax(dots, dim = -1)\n",
    "\n",
    "        # multply attention score * value\n",
    "        out = torch.einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        # rearrange as (b,n,h*d) => n_heads * d_head\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out =  self.to_out(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim : int, depth : int, n_heads : int, d_head : int, mlp_dim : int, dropout : float = 0.0):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim=dim, n_heads = n_heads, d_head = d_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim=dim, hidden_dim = mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x : torch.Tensor)->torch.Tensor:\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return self.norm(x)\n",
    "\n",
    "class ViViT(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        image_size : int, \n",
    "        patch_size : int, \n",
    "        n_frames : int, \n",
    "        dim : int = 192, \n",
    "        depth : int = 4, \n",
    "        n_heads : int = 3, \n",
    "        pool : str = 'cls', \n",
    "        in_channels : int = 3, \n",
    "        d_head :int = 64, \n",
    "        dropout : float = 0.,\n",
    "        embedd_dropout : float = 0., \n",
    "        scale_dim :int = 4, \n",
    "        ):\n",
    "        super(ViViT, self).__init__()\n",
    "        \n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls(cls token) or mean(mean pooling)'\n",
    "        assert image_size % patch_size == 0, \"Image dimension(height and width) must be divisible by the patch_size\"\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.n_frames = n_frames\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        n_patches = (image_size // patch_size) ** 2\n",
    "        patch_dim = in_channels * patch_size ** 2\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b t c (h p1) (w p2) -> b t (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),\n",
    "            nn.Linear(patch_dim, dim)\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, n_frames, n_patches + 1, dim))\n",
    "        self.space_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.space_transformer = Transformer(\n",
    "            dim, depth, n_heads, d_head, dim * scale_dim, dropout\n",
    "        )\n",
    "\n",
    "        self.temporal_token = nn.Parameter(torch.randn(1,1,dim))\n",
    "        self.temporal_transformer = Transformer(\n",
    "            dim, depth, n_heads, d_head, dim * scale_dim, dropout\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(embedd_dropout)\n",
    "        self.pool = pool\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x : torch.Tensor)->torch.Tensor:\n",
    "\n",
    "        if x.size()[1] == self.in_channels:\n",
    "            x = torch.permute(x, (0,2,1,3,4))\n",
    "            \n",
    "        x = self.to_patch_embedding(x)\n",
    "        b, t, n, _ = x.shape\n",
    "        cls_space_token = repeat(self.space_token, '() n d -> b t n d', b = b, t = t)\n",
    "        cls_temporal_token = repeat(self.temporal_token, '() n d -> b n d', b = b)\n",
    "\n",
    "        x = torch.cat((cls_space_token, x), dim = 2)\n",
    "        x += self.pos_embedding[:,:,:(n+1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = rearrange(x, 'b t n d -> (b t) n d')\n",
    "        x = self.space_transformer(x)\n",
    "        x = rearrange(x[:,0], '(b t) ... -> b t ...', b = b)\n",
    "\n",
    "        x = torch.cat((cls_temporal_token, x), dim = 1)\n",
    "        x = self.temporal_transformer(x)\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "        x = self.mlp(x)\n",
    "        x = torch.sigmoid(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def summary(self, device : str = 'cpu', show_input : bool = True, show_hierarchical : bool = True, print_summary : bool = True, show_parent_layers : bool = True)->None:\n",
    "        sample = torch.zeros((1, self.n_frames, self.in_channels, self.image_size, self.image_size), device = device)\n",
    "        return summary(self, sample, show_input = show_input, show_hierarchical=show_hierarchical, print_summary = print_summary, show_parent_layers=show_parent_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "\n",
    "def train_per_epoch(\n",
    "    train_loader : torch.utils.data.DataLoader, \n",
    "    model : torch.nn.Module,\n",
    "    optimizer : torch.optim.Optimizer,\n",
    "    scheduler : Optional[torch.optim.lr_scheduler._LRScheduler],\n",
    "    loss_fn : torch.nn.Module,\n",
    "    device : str = \"cpu\",\n",
    "    max_norm_grad : Optional[float] = None\n",
    "    ):\n",
    "\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "\n",
    "    total_pred = np.array([])\n",
    "    total_label = np.array([])\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        target = target.float().to(device)\n",
    "\n",
    "        output = model(data).squeeze(dim = 1)\n",
    "\n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # use gradient clipping\n",
    "        if max_norm_grad:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm_grad)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        pred = (output > torch.FloatTensor([0.5]).to(device))\n",
    "        train_acc += pred.eq(target.view_as(pred)).sum().item() / data.size(0) \n",
    "        \n",
    "        total_pred = np.concatenate((total_pred, pred.cpu().numpy().reshape(-1,)))\n",
    "        total_label = np.concatenate((total_label, target.cpu().numpy().reshape(-1,)))\n",
    "        \n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "\n",
    "    train_loss /= (batch_idx + 1)\n",
    "    train_acc /= (batch_idx + 1)\n",
    "\n",
    "    train_f1 = f1_score(total_label, total_pred, average = \"macro\")\n",
    "\n",
    "    return train_loss, train_acc, train_f1\n",
    "\n",
    "def valid_per_epoch(\n",
    "    valid_loader : torch.utils.data.DataLoader, \n",
    "    model : torch.nn.Module,\n",
    "    optimizer : torch.optim.Optimizer,\n",
    "    loss_fn : torch.nn.Module,\n",
    "    device : str = \"cpu\",\n",
    "    ):\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    valid_loss = 0\n",
    "    valid_acc = 0\n",
    "\n",
    "    total_pred = np.array([])\n",
    "    total_label = np.array([])\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "        with torch.no_grad():\n",
    "            optimizer.zero_grad()\n",
    "            data = data.to(device)\n",
    "            target = target.float().to(device)\n",
    "            output = model(data).squeeze(dim = 1)\n",
    "\n",
    "            loss = loss_fn(output, target)\n",
    "    \n",
    "            valid_loss += loss.item()\n",
    "            pred = (output >= torch.FloatTensor([0.5]).to(device))\n",
    "            valid_acc += pred.eq(target.view_as(pred)).sum().item() / data.size(0) \n",
    "\n",
    "            total_pred = np.concatenate((total_pred, pred.cpu().numpy().reshape(-1,)))\n",
    "            total_label = np.concatenate((total_label, target.cpu().numpy().reshape(-1,)))\n",
    "\n",
    "    valid_loss /= (batch_idx + 1)\n",
    "    valid_acc /= (batch_idx + 1)\n",
    "\n",
    "    valid_f1 = f1_score(total_label, total_pred, average = \"macro\")\n",
    "\n",
    "    return valid_loss, valid_acc, valid_f1\n",
    "\n",
    "def train(\n",
    "    train_loader : torch.utils.data.DataLoader, \n",
    "    valid_loader : torch.utils.data.DataLoader,\n",
    "    model : torch.nn.Module,\n",
    "    optimizer : torch.optim.Optimizer,\n",
    "    scheduler : Optional[torch.optim.lr_scheduler._LRScheduler],\n",
    "    loss_fn = None,\n",
    "    device : str = \"cpu\",\n",
    "    num_epoch : int = 64,\n",
    "    verbose : Optional[int] = 8,\n",
    "    save_best_only : bool = False,\n",
    "    save_best_dir : str = \"./weights/best.pt\",\n",
    "    save_last_dir : str = \"./weights/last.pt\",\n",
    "    max_norm_grad : Optional[float] = None,\n",
    "    criteria : str = \"f1_score\"\n",
    "    ):\n",
    "\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    \n",
    "    train_acc_list = []\n",
    "    valid_acc_list = []\n",
    "\n",
    "    train_f1_list = []\n",
    "    valid_f1_list = []\n",
    "\n",
    "    best_acc = 0\n",
    "    best_epoch = 0\n",
    "    best_f1 = 0\n",
    "    best_loss = torch.inf\n",
    "\n",
    "    if loss_fn is None:\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(reduction = 'mean')\n",
    "\n",
    "    for epoch in tqdm(range(num_epoch), desc = \"training process\"):\n",
    "\n",
    "        train_loss, train_acc, train_f1 = train_per_epoch(\n",
    "            train_loader, \n",
    "            model,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            loss_fn,\n",
    "            device,\n",
    "            max_norm_grad\n",
    "        )\n",
    "\n",
    "        valid_loss, valid_acc, valid_f1 = valid_per_epoch(\n",
    "            valid_loader, \n",
    "            model,\n",
    "            optimizer,\n",
    "            loss_fn,\n",
    "            device \n",
    "        )\n",
    "\n",
    "        train_loss_list.append(train_loss)\n",
    "        valid_loss_list.append(valid_loss)\n",
    "\n",
    "        train_acc_list.append(train_acc)\n",
    "        valid_acc_list.append(valid_acc)\n",
    "\n",
    "        train_f1_list.append(train_f1)\n",
    "        valid_f1_list.append(valid_f1)\n",
    "\n",
    "        if verbose:\n",
    "            if epoch % verbose == 0:\n",
    "                print(\"epoch : {}, train loss : {:.3f}, valid loss : {:.3f}, train acc : {:.3f}, valid acc : {:.3f}, train f1 : {:.3f}, valid f1 : {:.3f}\".format(\n",
    "                    epoch+1, train_loss, valid_loss, train_acc, valid_acc, train_f1, valid_f1\n",
    "                ))\n",
    "\n",
    "        # save the best parameters\n",
    "        if save_best_only:\n",
    "            if criteria == \"acc\" and best_acc < valid_acc:\n",
    "                best_acc = valid_acc\n",
    "                best_f1 = valid_f1\n",
    "                best_loss = valid_loss\n",
    "                best_epoch  = epoch\n",
    "                torch.save(model.state_dict(), save_best_dir)\n",
    "            elif criteria == \"f1_score\" and best_f1 < valid_f1:\n",
    "                best_acc = valid_acc\n",
    "                best_f1 = valid_f1\n",
    "                best_loss = valid_loss\n",
    "                best_epoch  = epoch\n",
    "                torch.save(model.state_dict(), save_best_dir)\n",
    "            elif criteria == \"loss\" and best_loss > valid_loss:\n",
    "                best_acc = valid_acc\n",
    "                best_f1 = valid_f1\n",
    "                best_loss = valid_loss\n",
    "                best_epoch  = epoch\n",
    "                torch.save(model.state_dict(), save_best_dir)\n",
    "\n",
    "        # save the last parameters\n",
    "        torch.save(model.state_dict(), save_last_dir)\n",
    "\n",
    "    # print(\"\\n============ Report ==============\\n\")\n",
    "    print(\"training process finished, best loss : {:.3f} and best acc : {:.3f}, best f1 : {:.3f}, best epoch : {}\".format(\n",
    "        best_loss, best_acc, best_f1, best_epoch\n",
    "    ))\n",
    "\n",
    "    return  train_loss_list, train_acc_list, train_f1_list,  valid_loss_list,  valid_acc_list, valid_f1_list\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    test_loader : torch.utils.data.DataLoader, \n",
    "    model : torch.nn.Module,\n",
    "    optimizer : Optional[torch.optim.Optimizer],\n",
    "    loss_fn = None,\n",
    "    device : Optional[str] = \"cpu\",\n",
    "    save_dir : Optional[str] = None\n",
    "):\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    total_pred = np.array([])\n",
    "    total_label = np.array([])\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for idx, (data, target) in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            optimizer.zero_grad()\n",
    "            data = data.to(device)\n",
    "            target = target.float().to(device)\n",
    "            output = model(data).squeeze(dim = 1)\n",
    "\n",
    "            loss = loss_fn(output, target)\n",
    "    \n",
    "            test_loss += loss.item()\n",
    "            pred = (output > torch.FloatTensor([0.5]).to(device))\n",
    "            test_acc += pred.eq(target.view_as(pred)).sum().item() / data.size(0) \n",
    "\n",
    "            total_pred = np.concatenate((total_pred, pred.cpu().numpy().reshape(-1,)))\n",
    "            total_label = np.concatenate((total_label, target.cpu().numpy().reshape(-1,)))\n",
    "\n",
    "    test_loss /= (idx + 1)\n",
    "    test_acc /= (idx + 1)\n",
    "    \n",
    "    test_f1 = f1_score(total_label, total_pred, average = \"macro\")\n",
    "    conf_mat = confusion_matrix(total_label,  total_pred)\n",
    "\n",
    "    plt.figure()\n",
    "    sns.heatmap(\n",
    "        conf_mat / np.sum(conf_mat, axis = 1)[:, None],\n",
    "        annot = True,\n",
    "        fmt = '.2f',\n",
    "        cmap = 'Blues',\n",
    "        xticklabels=[\"disruption\", \"normal\"],\n",
    "        yticklabels=[\"disruption\", \"normal\"]\n",
    "    )\n",
    "\n",
    "    plt.savefig(\"./results/confusion_matrix.png\")\n",
    "\n",
    "    print(\"############### Classification Report ####################\")\n",
    "    print(classification_report(total_label, total_pred, labels = [0,1]))\n",
    "    print(\"\\n# test acc : {:.2f}, test f1 : {:.2f}, test loss : {:.3f}\".format(test_acc, test_f1, test_loss))\n",
    "    print(conf_mat)\n",
    "\n",
    "    if save_dir:\n",
    "        with open(save_dir, 'w') as f:\n",
    "            f.write(classification_report(total_label, total_pred, labels = [0,1]))\n",
    "            summary = \"\\n# test score : {:.2f}, test loss : {:.3f}, test f1 : {:.3f}\".format(test_acc, test_loss, test_f1)\n",
    "            f.write(summary)\n",
    "\n",
    "    return test_loss, test_acc, test_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_focal_loss(inputs:torch.Tensor, gamma:float):\n",
    "    p = torch.exp(-inputs)\n",
    "    loss = (1-p) ** gamma * inputs\n",
    "    return loss.mean()\n",
    "\n",
    "class FocalLossLDAM(torch.nn.Module):\n",
    "    def __init__(self, weight : Optional[torch.Tensor] = None, gamma : float = 0.1):\n",
    "        super(FocalLossLDAM, self).__init__()\n",
    "        assert gamma >= 0, \"gamma should be positive\"\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, input : torch.Tensor, target : torch.Tensor)->torch.Tensor:\n",
    "        return compute_focal_loss(torch.nn.BCELoss()(input, target), self.gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch device avaliable :  True\n",
      "torch current device :  0\n",
      "torch device num :  2\n",
      "video dataset directory is not valid\n",
      "Number of train videos: 6374\n",
      "Number of val videos: 1595\n",
      "Number of test videos: 1994\n",
      "----------------------------------------------------------------------------\n",
      "      Layer (type)              Input Shape         Param #     Tr. Param #\n",
      "============================================================================\n",
      "       Rearrange-1     [1, 21, 3, 224, 224]               0               0\n",
      "          Linear-2        [1, 21, 196, 768]          49,216          49,216\n",
      "         Dropout-3         [1, 21, 197, 64]               0               0\n",
      "     Transformer-4            [21, 197, 64]         395,904         395,904\n",
      "     Transformer-5              [1, 22, 64]         395,904         395,904\n",
      "       LayerNorm-6                  [1, 64]             128             128\n",
      "          Linear-7                  [1, 64]              65              65\n",
      "============================================================================\n",
      "Total params: 841,217\n",
      "Trainable params: 841,217\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "augmentation_args = {\n",
    "    \"bright_val\" : 30,\n",
    "    \"bright_p\" : 0.5,\n",
    "    \"contrast_min\" : 1,\n",
    "    \"contrast_max\" : 1.5,\n",
    "    \"contrast_p\" : 0.5,\n",
    "    \"blur_k\" : 5,\n",
    "    \"blur_p\" : 0.5,\n",
    "    \"flip_p\" : 0.5,\n",
    "    \"vertical_ratio\" : 0.2,\n",
    "    \"vertical_p\" : 0.5,\n",
    "    \"horizontal_ratio\" : 0.2,\n",
    "    \"horizontal_p\" : 0.5\n",
    "}\n",
    "\n",
    "image_size = 224\n",
    "patch_size = 16\n",
    "\n",
    "use_focal_loss = True\n",
    "use_sampler = True\n",
    "\n",
    "batch_size = 16\n",
    "lr = 1e-3\n",
    "clip_len = 21\n",
    "num_epoch = 4\n",
    "verbose = 1\n",
    "gamma = 0.95\n",
    "save_best_dir = \"./weights/ViViT_clip_21_dist_0_best.pt\"\n",
    "save_last_dir = \"./weights/ViViT_clip_21_dist_0_last.pt\"\n",
    "save_result_dir = \"./results/train_valid_loss_acc_ViViT_clip_21_dist_0.png\"\n",
    "save_test_result = \"./results/test_ViViT_clip_21_dist_0.txt\"\n",
    "dataset = \"dur0.1_dis0\"\n",
    "\n",
    "# torch device state\n",
    "print(\"torch device avaliable : \", torch.cuda.is_available())\n",
    "print(\"torch current device : \", torch.cuda.current_device())\n",
    "print(\"torch device num : \", torch.cuda.device_count())\n",
    "\n",
    "# torch cuda initialize and clear cache\n",
    "torch.cuda.init()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# device allocation\n",
    "if(torch.cuda.device_count() >= 1):\n",
    "    device = \"cuda:\" + str(0)\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    \n",
    "# dataset composition\n",
    "import os\n",
    "try:\n",
    "    path = \"./dataset/\" + dataset + \"/\"\n",
    "    path_disruption = path + \"disruption/\"\n",
    "    path_borderline = path + \"borderline/\"\n",
    "    path_normal = path + \"normal/\"\n",
    "\n",
    "    dir_disruption_list = os.listdir(path_disruption)\n",
    "    dir_borderline_list = os.listdir(path_borderline)\n",
    "    dir_normal_list = os.listdir(path_normal)\n",
    "\n",
    "    print(\"disruption : \", len(dir_disruption_list))\n",
    "    print(\"normal : \", len(dir_normal_list))\n",
    "    print(\"borderline : \", len(dir_borderline_list))\n",
    "except:\n",
    "    print(\"video dataset directory is not valid\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    train_data = VideoDataset(dataset = dataset, split = \"train\", clip_len = clip_len, preprocess = False, augmentation = True, augmentation_args=augmentation_args)\n",
    "    valid_data = VideoDataset(dataset = dataset, split = \"val\", clip_len = clip_len, preprocess = False, augmentation=True, augmentation_args=augmentation_args)\n",
    "    test_data = VideoDataset(dataset = dataset, split = \"test\", clip_len = clip_len, preprocess = False, augmentation=False)\n",
    "\n",
    "    if use_sampler:\n",
    "        train_sampler = ImbalancedDatasetSampler(train_data)\n",
    "        valid_sampler = ImbalancedDatasetSampler(valid_data)\n",
    "        test_sampler = None\n",
    "\n",
    "    else:\n",
    "        train_sampler = None\n",
    "        valid_sampler = None\n",
    "        test_sampler = None\n",
    "    \n",
    "    train_loader = DataLoader(train_data, batch_size, sampler=train_sampler, num_workers = 8)\n",
    "    valid_loader = DataLoader(valid_data, batch_size, sampler=valid_sampler, num_workers = 8)\n",
    "    test_loader = DataLoader(test_data, batch_size, sampler=test_sampler, num_workers = 8)\n",
    "\n",
    "    sample_data, sample_label = next(iter(train_loader))\n",
    "\n",
    "    model = ViViT(\n",
    "        image_size = image_size,\n",
    "        patch_size = patch_size,\n",
    "        n_frames = clip_len,\n",
    "        dim = 64,\n",
    "        depth = 4,\n",
    "        n_heads = 4,\n",
    "        pool = \"cls\",\n",
    "        in_channels = 3,\n",
    "        d_head = 64,\n",
    "        dropout = 0.25,\n",
    "        embedd_dropout=0.25,\n",
    "        scale_dim = 4\n",
    "    )\n",
    "\n",
    "    model.cpu()\n",
    "    model.to(device)\n",
    "\n",
    "    model.summary(device, show_input = True, show_hierarchical=False, print_summary=True, show_parent_layers=False)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr = lr, weight_decay=gamma)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size = 2,\n",
    "            gamma = gamma\n",
    "    )\n",
    "    \n",
    "    if use_focal_loss:\n",
    "        loss_fn = FocalLossLDAM(weight = None, gamma = 0.5)\n",
    "    else: \n",
    "        # loss_fn = torch.nn.CrossEntropyLoss(reduction = \"mean\")\n",
    "        loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "    # training process\n",
    "    # train_loss,  train_acc, train_f1, valid_loss, valid_acc, valid_f1 = train(\n",
    "    #     train_loader,\n",
    "    #     valid_loader,\n",
    "    #     model,\n",
    "    #     optimizer,\n",
    "    #     scheduler,\n",
    "    #     loss_fn,\n",
    "    #     device,\n",
    "    #     num_epoch,\n",
    "    #     verbose,\n",
    "    #     save_best_only = False,\n",
    "    #     save_best_dir = save_best_dir,\n",
    "    #     save_last_dir = save_last_dir,\n",
    "    #     max_norm_grad = 5.0,\n",
    "    #     criteria = \"f1_score\"\n",
    "    #     )\n",
    "\n",
    "    # model.load_state_dict(torch.load(save_last_dir))\n",
    "\n",
    "    # # evaluation process\n",
    "    # test_loss, test_acc, test_f1 = evaluate(\n",
    "    #     test_loader,\n",
    "    #     model,\n",
    "    #     optimizer,\n",
    "    #     loss_fn,\n",
    "    #     device,\n",
    "    #     save_test_result\n",
    "    # )\n",
    "\n",
    "    model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset :  torch.Size([869, 3, 21, 224, 224])\n",
      "thermal quench : 87.89\n",
      "current quench : 87.91\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViViT(\n",
       "  (to_patch_embedding): Sequential(\n",
       "    (0): Rearrange('b t c (h p1) (w p2) -> b t (h w) (p1 p2 c)', p1=16, p2=16)\n",
       "    (1): Linear(in_features=768, out_features=64, bias=True)\n",
       "  )\n",
       "  (space_transformer): Transformer(\n",
       "    (layers): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (fn): Attention(\n",
       "            (to_qkv): Linear(in_features=64, out_features=768, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "              (1): Dropout(p=0.25, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.25, inplace=False)\n",
       "              (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "              (4): Dropout(p=0.25, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (fn): Attention(\n",
       "            (to_qkv): Linear(in_features=64, out_features=768, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "              (1): Dropout(p=0.25, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.25, inplace=False)\n",
       "              (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "              (4): Dropout(p=0.25, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (fn): Attention(\n",
       "            (to_qkv): Linear(in_features=64, out_features=768, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "              (1): Dropout(p=0.25, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.25, inplace=False)\n",
       "              (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "              (4): Dropout(p=0.25, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (fn): Attention(\n",
       "            (to_qkv): Linear(in_features=64, out_features=768, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "              (1): Dropout(p=0.25, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.25, inplace=False)\n",
       "              (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "              (4): Dropout(p=0.25, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (temporal_transformer): Transformer(\n",
       "    (layers): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (fn): Attention(\n",
       "            (to_qkv): Linear(in_features=64, out_features=768, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "              (1): Dropout(p=0.25, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.25, inplace=False)\n",
       "              (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "              (4): Dropout(p=0.25, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (fn): Attention(\n",
       "            (to_qkv): Linear(in_features=64, out_features=768, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "              (1): Dropout(p=0.25, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.25, inplace=False)\n",
       "              (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "              (4): Dropout(p=0.25, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (fn): Attention(\n",
       "            (to_qkv): Linear(in_features=64, out_features=768, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "              (1): Dropout(p=0.25, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.25, inplace=False)\n",
       "              (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "              (4): Dropout(p=0.25, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): ModuleList(\n",
       "        (0): PreNorm(\n",
       "          (fn): Attention(\n",
       "            (to_qkv): Linear(in_features=64, out_features=768, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "              (1): Dropout(p=0.25, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): PreNorm(\n",
       "          (fn): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.25, inplace=False)\n",
       "              (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "              (4): Dropout(p=0.25, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       "  (mlp): Sequential(\n",
       "    (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_prob_curve(\n",
    "    dataset : torch.Tensor, \n",
    "    model : torch.nn.Module, \n",
    "    batch_size : int = 32, \n",
    "    device : str = \"cpu\", \n",
    "    save_dir : Optional[str] = \"./results/disruption_probs_curve.png\",\n",
    "    shot_list_dir : Optional[str] = \"./dataset/KSTAR_Disruption_Shot_List.csv\",\n",
    "    shot_number : Optional[int] = None,\n",
    "    clip_len : Optional[int] = None,\n",
    "    dist_frame : Optional[int] = None,\n",
    "    use_continuous_frame : bool = True\n",
    "    ):\n",
    "    prob_list = []\n",
    "    video_len = dataset.size(0)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    if video_len >= batch_size:\n",
    "        batch_rest = video_len % batch_size\n",
    "            \n",
    "        for idx in range(int(video_len / batch_size)):\n",
    "            with torch.no_grad():\n",
    "                idx_start = batch_size * idx\n",
    "                idx_end = batch_size * (idx + 1)\n",
    "\n",
    "                frames = dataset[idx_start : idx_end, :, :, :, :]\n",
    "                frames = frames.to(device)\n",
    "                output = model(frames)\n",
    "            \n",
    "                prob_list.extend(\n",
    "                    output.cpu().detach().numpy().reshape(-1,).tolist()\n",
    "                )\n",
    "        \n",
    "        if batch_rest !=0:\n",
    "            with torch.no_grad():\n",
    "                idx_start = batch_size * (idx + 1)\n",
    "                idx_end = idx_start + batch_rest\n",
    "\n",
    "                frames = dataset[idx_start : idx_end, :, :, :, :]\n",
    "                frames = frames.to(device)\n",
    "                output = model(frames)\n",
    "            \n",
    "                prob_list.extend(\n",
    "                    output.cpu().detach().numpy().reshape(-1,).tolist()\n",
    "                )\n",
    "\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            frames = dataset[:, :, :, :, :]\n",
    "            frames = frames.to(device)\n",
    "            output = model(frames)\n",
    "            prob_list.extend(\n",
    "                output.cpu().detach().numpy().reshape(-1,).tolist()\n",
    "            )\n",
    "     \n",
    "    if shot_list_dir and shot_number:\n",
    "        shot_list = pd.read_csv(shot_list_dir)\n",
    "        shot_info = shot_list[shot_list[\"shot\"] == shot_number]\n",
    "    else:\n",
    "        shot_list = None\n",
    "        shot_info = None\n",
    "\n",
    "    if shot_info is not None:\n",
    "        t_disrupt = shot_info[\"tTQend\"].values[0]\n",
    "        t_current = shot_info[\"tipminf\"].values[0]\n",
    "    else:\n",
    "        t_disrupt = None\n",
    "        t_current = None\n",
    "\n",
    "    if use_continuous_frame:\n",
    "        interval = 1\n",
    "        # clip_len + distance  \n",
    "        prob_list = [0] * (clip_len + dist_frame) + prob_list\n",
    "    else:\n",
    "        interval = clip_len\n",
    "        prob_list = [0] * (1 + int(dist_frame / clip_len)) + prob_list\n",
    "\n",
    "    if save_dir:\n",
    "        fps = 210\n",
    "\n",
    "        time_x = np.arange(1, len(prob_list) + 1) * (1/fps) * interval\n",
    "        threshold_line = [0.5] * len(time_x)\n",
    "\n",
    "        plt.figure(figsize = (8,5))\n",
    "        plt.plot(time_x, threshold_line, 'k', label = \"threshold(p = 0.5)\")\n",
    "        plt.plot(time_x, prob_list, 'b-', label = \"disruption probs\")\n",
    "\n",
    "        if t_disrupt is not None:\n",
    "            plt.axvline(x = t_disrupt, ymin = 0, ymax = 1, color = \"red\", linestyle = \"dashed\", label = \"thermal quench\")\n",
    "            print(\"thermal quench : {:.2f}\".format(t_disrupt))\n",
    "        \n",
    "        if t_current is not None:\n",
    "            plt.axvline(x = t_current, ymin = 0, ymax = 1, color = \"green\", linestyle = \"dashed\", label = \"current quench\")\n",
    "            print(\"current quench : {:.2f}\".format(t_current))\n",
    "\n",
    "        plt.ylabel(\"probability\")\n",
    "        plt.xlabel(\"time(unit : s)\")\n",
    "        plt.ylim([0,1])\n",
    "        plt.xlim([0,max(time_x)])\n",
    "        plt.legend()\n",
    "        plt.savefig(save_dir)\n",
    "\n",
    "    return prob_list\n",
    "\n",
    "from src.utils.utility import video2tensor\n",
    "\n",
    "video_path = \"./dataset/raw_videos/raw_videos/021757tv02.avi\"\n",
    "shot_list_dir = \"./dataset/KSTAR_Disruption_Shot_List.csv\"\n",
    "shot_number = 21757\n",
    "\n",
    "dataset = video2tensor(\n",
    "    dir = video_path,\n",
    "    channels  = 3, \n",
    "    clip_len  = clip_len, \n",
    "    crop_size  = 224,\n",
    "    resize_width  = 256,\n",
    "    resize_height = 256,\n",
    "    use_continuous_frame = False\n",
    ")\n",
    "\n",
    "print(\"dataset : \", dataset.size())\n",
    "\n",
    "# torch cuda initialize and clear cache\n",
    "torch.cuda.init()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# device allocation\n",
    "if(torch.cuda.device_count() >= 1):\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "model = ViViT(\n",
    "    image_size = image_size,\n",
    "    patch_size = patch_size,\n",
    "    n_frames = clip_len,\n",
    "    dim = 64,\n",
    "    depth = 4,\n",
    "    n_heads = 4,\n",
    "    pool = \"cls\",\n",
    "    in_channels = 3,\n",
    "    d_head = 64,\n",
    "    dropout = 0.25,\n",
    "    embedd_dropout=0.25,\n",
    "    scale_dim = 4\n",
    ")\n",
    "\n",
    "weight = \"./weights/ViViT_clip_21_dist_0_last.pt\"\n",
    "model.load_state_dict(torch.load(weight))\n",
    "\n",
    "probs = generate_prob_curve(dataset, model, batch_size = 16, device = device, shot_list_dir = shot_list_dir, shot_number = shot_number, clip_len = clip_len, dist_frame = 0, use_continuous_frame = False)\n",
    "model.cpu()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('research-env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b7bc8097f24747f72629445db54bed151603a8d63744e142002cb75630cca553"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
